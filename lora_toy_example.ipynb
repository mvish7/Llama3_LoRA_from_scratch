{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.parametrize as parametrize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 60000\n",
      "Validation dataset size: 10000\n",
      "Batch size: 64\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters for training\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 10\n",
    "\n",
    "# Define transformations for training and testing datasets\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((32, 32)), # Resize images to 32x32\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((32, 32)), # Resize images to 32x32\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load Fashion MNIST dataset\n",
    "train_dataset = datasets.FashionMNIST(root='./data', download=True, train=True, transform=transform_train)\n",
    "val_dataset = datasets.FashionMNIST(root='./data', download=True, train=False, transform=transform_train)\n",
    "\n",
    "\n",
    "\n",
    "# Create data loaders for training and validation sets\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"Training dataset size:\", len(train_dataset))\n",
    "print(\"Validation dataset size:\", len(val_dataset))\n",
    "print(\"Batch size:\", batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label map\n",
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters for training\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 10\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "\n",
    "# Define the CNN model\n",
    "class FashionMNISTCNN(nn.Module):\n",
    "    def __init__(self, h1, h2):\n",
    "        super(FashionMNISTCNN, self).__init__()\n",
    "        self.linear1 = nn.Linear(32*32, h1)\n",
    "        self.linear2 = nn.Linear(h1, h2)\n",
    "        self.linear3 = nn.Linear(h2, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_s = x.shape\n",
    "        x = x.reshape(x_s[0], x_s[1], x_s[-2]*x_s[-1])\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters in model - 3047010\n",
      "Epoch 1, Loss: 1.1668907664477952\n",
      "Epoch 2, Loss: 0.6404037685917893\n",
      "Epoch 3, Loss: 0.5464359337904814\n",
      "Epoch 4, Loss: 0.5001209350282958\n",
      "Epoch 5, Loss: 0.474462777868644\n",
      "Epoch 6, Loss: 0.45565528541739814\n",
      "Epoch 7, Loss: 0.4417266100486204\n",
      "Epoch 8, Loss: 0.42736764938465316\n",
      "Epoch 9, Loss: 0.4191315769831509\n",
      "Epoch 10, Loss: 0.41034741289834226\n"
     ]
    }
   ],
   "source": [
    "model = FashionMNISTCNN(h1=1000, h2=2000)\n",
    "model.to(\"cuda\")\n",
    "num_params = sum([p.numel() for p in model.parameters()])\n",
    "print(f\"Num parameters in model - {num_params}\")\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(\"cuda\")\n",
    "        labels = labels.to(\"cuda\")\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze(1)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss / (i + 1)}')\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.849\n",
      "wrong counts for the digit 0: 185\n",
      "wrong counts for the digit 1: 43\n",
      "wrong counts for the digit 2: 220\n",
      "wrong counts for the digit 3: 125\n",
      "wrong counts for the digit 4: 253\n",
      "wrong counts for the digit 5: 74\n",
      "wrong counts for the digit 6: 440\n",
      "wrong counts for the digit 7: 59\n",
      "wrong counts for the digit 8: 49\n",
      "wrong counts for the digit 9: 67\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    wrong_counts = [0 for i in range(10)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            x, y = data\n",
    "            x = x.to(\"cuda\")\n",
    "            y = y.to(\"cuda\")\n",
    "            output = model(x)\n",
    "            for idx, i in enumerate(output):\n",
    "                if torch.argmax(i) == y[idx]:\n",
    "                    correct +=1\n",
    "                else:\n",
    "                    wrong_counts[y[idx]] +=1\n",
    "                total +=1\n",
    "    print(f'Accuracy: {round(correct/total, 3)}')\n",
    "    for i in range(len(wrong_counts)):\n",
    "        print(f'wrong counts for the digit {i}: {wrong_counts[i]}')\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameterization -- \n",
    "### What is it? Why it is necessary? How to do it?\n",
    "\n",
    "Model reparameterization is a technique of transforming model's parameters for certain operations e.g. Regularization. To reparametrize the model parameters, we need to  instruct pytorch on how we want to transform parameters of certain layers. One way of doing this is to implement the reparameterization by hand and then manually transforming the parameters. Below is an example of reparameterization that enforces orthogonality of weights ->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert model's parameter into orthogonal matrix\n",
    "def orthogonal_params(weights):\n",
    "  q, r = torch.linalg.qr(weights)\n",
    "  return q\n",
    "\n",
    "# let's define a simple model\n",
    "class SimpleModelManual(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(SimpleModelManual, self).__init__()\n",
    "        # Define an unconstrained parameter\n",
    "        self.weight_raw = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))  # Optional bias\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the orthogonal reparameterization manually\n",
    "        Q = orthogonal_params(self.weight_raw)  # QR decomposition to enforce orthogonality\n",
    "        # Perform linear transformation with reparameterized weight\n",
    "        return x @ Q.T + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reparameterized weight matrix (Q):\n",
      " tensor([[-0.6216, -0.2397, -0.6985,  0.2387, -0.1059],\n",
      "        [ 0.6800, -0.1291, -0.3929,  0.5752,  0.1889],\n",
      "        [ 0.0138,  0.9426, -0.3252, -0.0023, -0.0750],\n",
      "        [ 0.3062, -0.1319, -0.1589, -0.2023, -0.9070],\n",
      "        [ 0.2392, -0.1415, -0.4761, -0.7558,  0.3533]],\n",
      "       grad_fn=<LinalgQrBackward0>)\n",
      "Is orthogonal (Q^T Q = I):\n",
      " True\n"
     ]
    }
   ],
   "source": [
    "# let's check if our reparameterization works\n",
    "\n",
    "# Model parameters\n",
    "in_features = 5\n",
    "out_features = 5\n",
    "\n",
    "# Create the model\n",
    "model = SimpleModelManual(in_features, out_features)\n",
    "\n",
    "# Input tensor\n",
    "x = torch.randn(3, in_features)\n",
    "\n",
    "# Forward pass\n",
    "output = model(x)\n",
    "\n",
    "# Check if the reparameterized weight matrix is orthogonal\n",
    "Q, _ = torch.linalg.qr(model.weight_raw)  # Apply QR to get orthogonal weight\n",
    "print(\"Reparameterized weight matrix (Q):\\n\", Q)\n",
    "print(\"Is orthogonal (Q^T Q = I):\\n\", torch.allclose(Q.T @ Q, torch.eye(out_features), atol=1e-6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In manual implementation of reparameterization, we had to reimplement the linear transformation which is readially available in pytorch as nn.Linear(). It will become tedious to say the least or even impossible to reimplement several layers for such manual implementatins. \n",
    "\n",
    "Luckily, pytorch has made life easier once again, and such reparameterizations are made easy using `register_parametrization`. Below is an example of Pytorch's register_parametrization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrthogonalParam(nn.Module):\n",
    "    def forward(self, W):\n",
    "        # Use QR decomposition to make W orthogonal\n",
    "        Q, R = torch.linalg.qr(W)\n",
    "        return Q\n",
    "\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        \n",
    "        # Apply orthogonal parameterization to the weight of the linear layer\n",
    "        parametrize.register_parametrization(self.linear, \"weight\", OrthogonalParam())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight matrix:\n",
      " tensor([[-0.5994, -0.3149,  0.1313,  0.6633, -0.2904],\n",
      "        [ 0.3432, -0.3821, -0.3499, -0.1396, -0.7710],\n",
      "        [ 0.4510,  0.6129, -0.0295,  0.6162, -0.2012],\n",
      "        [-0.5445,  0.6158, -0.1332, -0.3582, -0.4222],\n",
      "        [-0.1521, -0.0084, -0.9175,  0.1804,  0.3202]],\n",
      "       grad_fn=<LinalgQrBackward0>)\n",
      "Is orthogonal (W^T W = I):\n",
      " True\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "in_features = 5\n",
    "out_features = 5\n",
    "\n",
    "# Create the model\n",
    "model = SimpleModel(in_features, out_features)\n",
    "\n",
    "# Input tensor\n",
    "x = torch.randn(3, in_features)\n",
    "\n",
    "# Forward pass\n",
    "output = model(x)\n",
    "\n",
    "# Verify orthogonality\n",
    "W = model.linear.weight  # Get the parameterized weight\n",
    "# W_original = model.linear.parametrizations.weight.original\n",
    "print(\"Weight matrix:\\n\", W)\n",
    "print(\"Is orthogonal (W^T W = I):\\n\", torch.allclose(W.T @ W, torch.eye(out_features), atol=1e-6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how easy it was to parameterize model when using register_parametrization. \n",
    "\n",
    "Now we need to apply same trick for LoRA.. Why?\n",
    "Because we need to transform the model's parameters to a rank deficiant matrix. i.e. we need to transform the paramters for certain operations..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRA(nn.Module):\n",
    "  def __init__(self, in_feat, out_feat, rank, alpha, device=\"cuda\") -> None:\n",
    "    super(LoRA, self).__init__()\n",
    "\n",
    "    # as per paper, we need to break down ∆W into A and B matrices such that ∆W = BA. Additionally ∆W = BA should be zeros at the beginning \n",
    "    # The A matrix is initialized randomly from a normal distribution, and B is all zeros\n",
    "\n",
    "    self.lora_b = nn.Parameter(torch.zeros(in_feat, rank)).to(device)\n",
    "    self.lora_a = nn.Parameter(torch.zeros(rank, out_feat)).to(device)\n",
    "    nn.init.normal_(self.lora_a, mean=0, std=1)\n",
    "\n",
    "    # scaling the ∆W by using alpha -- section 4.1 in paper \"We then scale ∆Wx by α/r , where α is a constant in r\"\n",
    "    self.scale = alpha / rank\n",
    "    self.enabled = True\n",
    "\n",
    "  def forward(self, original_w):\n",
    "    delta_w = torch.matmul(self.lora_b, self.lora_a).view(original_w.shape) * self.scale\n",
    "    return original_w "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, let's add this reparametrization to out FashionMNIST network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parametrize_model(curr_layer, device, rank=1, lora_alpha=1):\n",
    "  \"\"\"adds matirx A and B of LoRA to each liner layer\n",
    "  \"\"\"\n",
    "  in_feat, out_feat = curr_layer.weight.shape\n",
    "  return LoRA(in_feat, out_feat, rank, lora_alpha, device)\n",
    "\n",
    "parametrize.register_parametrization(\n",
    "    model.linear1, \"weight\", parametrize_model(model.linear1, \"cuda\")\n",
    ")\n",
    "parametrize.register_parametrization(\n",
    "    model.linear2, \"weight\", parametrize_model(model.linear2, \"cuda\")\n",
    ")\n",
    "parametrize.register_parametrization(\n",
    "    model.linear3, \"weight\", parametrize_model(model.linear3, \"cuda\")\n",
    ")\n",
    "\n",
    "\n",
    "def enable_disable_lora(enabled=True):\n",
    "    for layer in [model.linear1, model.linear2, model.linear3]:\n",
    "        layer.parametrizations[\"weight\"][0].enabled = enabled\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters (original): 3,047,010\n",
      "Total number of parameters (original + LoRA): 3,054,044\n",
      "Parameters introduced by LoRA: 7,034\n",
      "Parameters incremment: 0.231%\n"
     ]
    }
   ],
   "source": [
    "total_parameters_lora = 0\n",
    "total_parameters_non_lora = 0\n",
    "\n",
    "for index, layer in enumerate([model.linear1, model.linear2, model.linear3]):\n",
    "    total_parameters_lora += layer.parametrizations[\"weight\"][0].lora_a.nelement() + \\\n",
    "    layer.parametrizations[\"weight\"][0].lora_b.nelement()\n",
    "    total_parameters_non_lora += layer.weight.nelement() + layer.bias.nelement()\n",
    "    \n",
    "# The non-LoRA parameters count must match the original network\n",
    "assert total_parameters_non_lora == num_params\n",
    "print(f'Total number of parameters (original): {total_parameters_non_lora:,}')\n",
    "print(f'Total number of parameters (original + LoRA): {total_parameters_lora + total_parameters_non_lora:,}')\n",
    "print(f'Parameters introduced by LoRA: {total_parameters_lora:,}')\n",
    "parameters_incremment = (total_parameters_lora / total_parameters_non_lora) * 100\n",
    "print(f'Parameters incremment: {parameters_incremment:.3f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finetune our model on worst performing class i.e. cls_id = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing non-LoRA parameter linear1.bias\n",
      "Freezing non-LoRA parameter linear1.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter linear2.bias\n",
      "Freezing non-LoRA parameter linear2.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter linear3.bias\n",
      "Freezing non-LoRA parameter linear3.parametrizations.weight.original\n"
     ]
    }
   ],
   "source": [
    "# Freeze the non-Lora parameters\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lora' not in name:\n",
    "        print(f'Freezing non-LoRA parameter {name}')\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset again, by keeping only the digit 9\n",
    "mnist_trainset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform_train)\n",
    "exclude_indices = mnist_trainset.targets == 6\n",
    "mnist_trainset.data = mnist_trainset.data[exclude_indices]\n",
    "mnist_trainset.targets = mnist_trainset.targets[exclude_indices]\n",
    "# Create a dataloader for the training\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.9729501763556866\n",
      "Epoch 2, Loss: 0.9728141014880323\n",
      "Epoch 3, Loss: 0.973069206831303\n",
      "Epoch 4, Loss: 0.9727423666639531\n",
      "Epoch 5, Loss: 0.9722493982061426\n",
      "Epoch 6, Loss: 0.9724092033315213\n",
      "Epoch 7, Loss: 0.9729931601818572\n",
      "Epoch 8, Loss: 0.9727127234986488\n",
      "Epoch 9, Loss: 0.9728179035034585\n",
      "Epoch 10, Loss: 0.9722128377315846\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(2):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(\"cuda\")\n",
    "        labels = labels.to(\"cuda\")\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze(1)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.requires_grad = True\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss / (i + 1)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.849\n",
      "wrong counts for the digit 0: 185\n",
      "wrong counts for the digit 1: 43\n",
      "wrong counts for the digit 2: 220\n",
      "wrong counts for the digit 3: 125\n",
      "wrong counts for the digit 4: 253\n",
      "wrong counts for the digit 5: 74\n",
      "wrong counts for the digit 6: 440\n",
      "wrong counts for the digit 7: 59\n",
      "wrong counts for the digit 8: 49\n",
      "wrong counts for the digit 9: 67\n"
     ]
    }
   ],
   "source": [
    "# testing fine-tuned model\n",
    "\n",
    "enable_disable_lora(enabled=False)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
